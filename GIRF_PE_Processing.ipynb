{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to process the .dat file optained from the scanner \n",
    "This shoudl be run 1 direction at a time \n",
    "\n",
    "The intial loading of the file takes around 10 minutes per direction - this only needs to be ran once for coil combination - no significant run times after this\n",
    "This file performs the coil combination based on the reference data and then separates the data into the relevant .mat files\n",
    "Remember to add the InputGradients.mat file into the folder for the GIRF calcualtion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mapvbvd import mapVBVD as mapVBVD\n",
    "import matplotlib.pyplot as plt\n",
    "import twixtools \n",
    "from fsl_mrs.utils.preproc import combine\n",
    "from scipy.io import savemat\n",
    "from fsl_mrs.utils.preproc.combine import svd_reduce, weightedCombination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mri_data(pattern, filename, start_idx=0, num_scans_to_load=3500):\n",
    "    # Load the Twix file with parsed data\n",
    "    multi_twix = twixtools.read_twix(filename, parse_data=True)\n",
    "    \n",
    "    # Extract the subset of mdb objects\n",
    "    end_idx = start_idx + num_scans_to_load\n",
    "    mdb_data_subset = multi_twix[-1]['mdb'][start_idx:end_idx]\n",
    "    \n",
    "    # Initialize a list to hold the gradient data\n",
    "    all_data = []\n",
    "    \n",
    "    # Extract data for the subset\n",
    "    for mdb in mdb_data_subset:\n",
    "        if mdb.is_image_scan():  # Check if it's an image scan\n",
    "            all_data.append(mdb.data)\n",
    "    \n",
    "    # Convert the list to a NumPy array\n",
    "    all_data = np.asarray(all_data)\n",
    "    \n",
    "    # Reshape and transpose data\n",
    "    #This is because the 80000 data points are collected as 10 groups of 8000\n",
    "    ungrouped = all_data.reshape(350, 10, 32, 8000)\n",
    "    ungroupedT = np.transpose(ungrouped, (0, 2, 1, 3))\n",
    "    gradient_data = ungroupedT.reshape(350, 32, 80000)\n",
    "    data = np.transpose(gradient_data, (0, 2, 1))\n",
    "    \n",
    "    # Split data into reference and triangle groups\n",
    "    data_ref = []\n",
    "    data_tri = []\n",
    "    \n",
    "    RRef = 1 #Number of repeats which shouldve been left as 1\n",
    "    RTri = 1\n",
    "    chunk_size = 50 * RRef + 300 * RTri\n",
    "    for i in range(0, np.shape(data)[0], chunk_size):\n",
    "        data_ref.append(data[i:i + 50 * RRef, :, :])\n",
    "        data_tri.append(data[i + 50 * RRef:i + chunk_size, :, :])\n",
    "    \n",
    "    data_ref = np.vstack(data_ref)\n",
    "    data_tri = np.vstack(data_tri)\n",
    "    \n",
    "    # Process reference data\n",
    "    averaged_data_ref = data_ref.reshape(50, RRef, 80000, 32).mean(axis=1)\n",
    "    \n",
    "    #Apply coil combination to the reference data and calcuate the weights for the triangle data \n",
    "    data_ref_corrected = []\n",
    "    weightslist = []\n",
    "    for i in range(np.shape(averaged_data_ref)[0]):\n",
    "        combined, weights, _ = svd_reduce(averaged_data_ref[i], return_alpha=True)\n",
    "        data_ref_corrected.append(combined)\n",
    "        weightslist.append(weights)\n",
    "    \n",
    "    data_ref_corrected = np.array(data_ref_corrected)\n",
    "    weightslist = np.array(weightslist)\n",
    "    \n",
    "    # Apply weighting to triangle data\n",
    "    data_tri_corrected = []\n",
    "    for idx, element in enumerate(pattern):\n",
    "        fid = weightedCombination(data_tri[idx], weightslist[element])\n",
    "        data_tri_corrected.append(fid)\n",
    "    \n",
    "    data_tri_corrected = np.array(data_tri_corrected)\n",
    "    \n",
    "    return data_ref_corrected, data_tri_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to split the data into batches, perform the coil combination, then rejoin the batches together \n",
    "#Batch processing only performed to help with memory constraints running on a CPU\n",
    "\n",
    "def process_full_mri_data(pattern, filename):\n",
    "    total_scans = 21000\n",
    "    num_scans_to_load = 3500 #This is a reference and 3 different triangluar amplitudes \n",
    "    num_iterations = total_scans // num_scans_to_load\n",
    "    \n",
    "    data_ref_corrected_list = []\n",
    "    data_tri_corrected_list = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        data_ref_corrected, data_tri_corrected = process_mri_data(pattern, filename, start_idx=i * num_scans_to_load)\n",
    "        data_ref_corrected_list.append(data_ref_corrected)\n",
    "        data_tri_corrected_list.append(data_tri_corrected)\n",
    "    \n",
    "    final_data_ref_corrected = np.concatenate(data_ref_corrected_list, axis=0)\n",
    "    final_data_tri_corrected = np.concatenate(data_tri_corrected_list, axis=0)\n",
    "    \n",
    "    return final_data_ref_corrected, final_data_tri_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 2, 3, 2, 3, 4, 5, 4, 5, 6, 7, 6, 7, 8, 9, 8, 9, 10, 11, 10, 11, 12, 13, 12, 13, 14, 15, 14, 15, 16, 17, 16, 17, 18, 19, 18, 19, 20, 21, 20, 21, 22, 23, 22, 23, 24, 25, 24, 25, 26, 27, 26, 27, 28, 29, 28, 29, 30, 31, 30, 31, 32, 33, 32, 33, 34, 35, 34, 35, 36, 37, 36, 37, 38, 39, 38, 39, 40, 41, 40, 41, 42, 43, 42, 43, 44, 45, 44, 45, 46, 47, 46, 47, 48, 49, 48, 49, 0, 1, 0, 1, 2, 3, 2, 3, 4, 5, 4, 5, 6, 7, 6, 7, 8, 9, 8, 9, 10, 11, 10, 11, 12, 13, 12, 13, 14, 15, 14, 15, 16, 17, 16, 17, 18, 19, 18, 19, 20, 21, 20, 21, 22, 23, 22, 23, 24, 25, 24, 25, 26, 27, 26, 27, 28, 29, 28, 29, 30, 31, 30, 31, 32, 33, 32, 33, 34, 35, 34, 35, 36, 37, 36, 37, 38, 39, 38, 39, 40, 41, 40, 41, 42, 43, 42, 43, 44, 45, 44, 45, 46, 47, 46, 47, 48, 49, 48, 49, 0, 1, 0, 1, 2, 3, 2, 3, 4, 5, 4, 5, 6, 7, 6, 7, 8, 9, 8, 9, 10, 11, 10, 11, 12, 13, 12, 13, 14, 15, 14, 15, 16, 17, 16, 17, 18, 19, 18, 19, 20, 21, 20, 21, 22, 23, 22, 23, 24, 25, 24, 25, 26, 27, 26, 27, 28, 29, 28, 29, 30, 31, 30, 31, 32, 33, 32, 33, 34, 35, 34, 35, 36, 37, 36, 37, 38, 39, 38, 39, 40, 41, 40, 41, 42, 43, 42, 43, 44, 45, 44, 45, 46, 47, 46, 47, 48, 49, 48, 49]\n"
     ]
    }
   ],
   "source": [
    "#Load in .dat file \n",
    "direction = 'z'\n",
    "filename = f'/Users/jamesbacon/Library/CloudStorage/OneDrive-Nexus365/GIRF_Correction/250313_pe_girf/meas_MID00184_FID46448_full_{direction}.dat'\n",
    "\n",
    "#The pattern is used so that the coil combination for the triangular data used the coil weights from the equivalent reference data\n",
    "\n",
    "def generate_pattern(limit):\n",
    "    pattern = []\n",
    "    for i in range(0, limit + 1, 2):\n",
    "        pattern.extend([i, i+1, i, i+1])\n",
    "    return pattern\n",
    "\n",
    "pattern = generate_pattern(48) # Goes from 0-49, this is the 25 phase encode directions * 2 slices (positive and negative offsets)\n",
    "pattern = pattern*3            # Multiply by 3 as there are 3 triangle amplitudes between each reference and contained in each batch \n",
    "print(pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software version: VD/VE (!?)\n",
      "\n",
      "Scan  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 3.66G/40.1G [00:00<00:08, 4.76GB/s]/opt/anaconda3/envs/fsl_mrs/lib/python3.11/site-packages/tqdm/std.py:1229: RuntimeWarning: overflow encountered in scalar add\n",
      "  self.n += n\n",
      "/opt/anaconda3/envs/fsl_mrs/lib/python3.11/site-packages/tqdm/std.py:1232: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  if self.n - self.last_print_n >= self.miniters:\n",
      "/opt/anaconda3/envs/fsl_mrs/lib/python3.11/site-packages/tqdm/std.py:1237: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  dn = self.n - self.last_print_n  # >= n\n",
      "  0%|          | 80.0M/40.1G [00:07<1:07:41, 10.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software version: VD/VE (!?)\n",
      "\n",
      "Scan  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 80.0M/40.1G [00:07<1:07:47, 10.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software version: VD/VE (!?)\n",
      "\n",
      "Scan  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 80.0M/40.1G [00:07<1:03:15, 11.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software version: VD/VE (!?)\n",
      "\n",
      "Scan  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 80.0M/40.1G [00:07<1:01:38, 11.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software version: VD/VE (!?)\n",
      "\n",
      "Scan  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 80.0M/40.1G [00:07<1:01:05, 11.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software version: VD/VE (!?)\n",
      "\n",
      "Scan  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 80.0M/40.1G [00:07<1:02:20, 11.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "data_ref_corrected, data_tri_corrected = process_full_mri_data(pattern=pattern, filename=filename) #Takes ~10 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 80000)\n",
      "(1800, 80000)\n"
     ]
    }
   ],
   "source": [
    "print(data_ref_corrected.shape)\n",
    "print(data_tri_corrected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ref = data_ref_corrected.reshape(6, 50, 80000)\n",
    "\n",
    "# Split into positive and negative y-slices (shape: 80000, 25, 6)\n",
    "dataposy = data_ref[:, 0::2, :].transpose(2, 1, 0)  # Even indices correspond to a positive slice offset \n",
    "datanegy = data_ref[:, 1::2, :].transpose(2, 1, 0)  # Odd indices correspond to a negative slice offset\n",
    "direction ='z'\n",
    "\n",
    "#Apply the 2D FT for the Phase encoding \n",
    "def ref_2dFT(input):\n",
    "    input_grid = input.reshape(80000, 5, 5, 6)\n",
    "    ft_kspace = ft_kspace = np.fft.fft2(input_grid, axes=(1, 2))\n",
    "    ft_kspace_shifted = np.fft.fftshift(ft_kspace, axes=(1, 2))\n",
    "    modified_kspace_all = ft_kspace_shifted.reshape(80000, 25, 6)\n",
    "\n",
    "    return modified_kspace_all\n",
    "\n",
    "dataposy_FT = ref_2dFT(dataposy)\n",
    "datanegy_FT = ref_2dFT(datanegy)\n",
    "\n",
    "# Experiment parameters\n",
    "RRef = 1\n",
    "RTri = 1\n",
    "acqNumRef = 6\n",
    "acqNumTri = 18 \n",
    "dwellTime = 5 #In microseconds\n",
    "\n",
    "gradAmp = np.array([\n",
    "    9, 10.8, 12.6, 14.4, 16.2, 18, 19.8, 21.6, 23.4, 25.2, \n",
    "    27, 28.8, 30.6, 32.4, 34.2, 36, 37.8, 39.6\n",
    "]).reshape(1, 18)\n",
    "nch = 1 #As we have alreayd performed coil combination \n",
    "roPts = 80000\n",
    "roTime = np.arange(0, dwellTime * roPts, dwellTime)\n",
    "\n",
    "# Save positive y-slice\n",
    "MatDataRef_pos = {\n",
    "    'acqNum': acqNumRef,\n",
    "    'avgNum': RRef,\n",
    "    'dwellTime': dwellTime,\n",
    "    'gradAmp': gradAmp,\n",
    "    'kspace_all': dataposy_FT,  \n",
    "    'nch': nch,\n",
    "    'roPts': roPts,\n",
    "    'roTime': roTime\n",
    "}\n",
    "filename_pos = f\"/Users/jamesbacon/Library/CloudStorage/OneDrive-Nexus365/GIRF_Correction/250313_pe_girf/GIRF_PE/Ref+{direction}slice.mat\"\n",
    "#savemat(filename_pos, MatDataRef_pos)\n",
    "\n",
    "# Save negative y-slice\n",
    "MatDataRef_neg = {\n",
    "    'acqNum': acqNumRef,\n",
    "    'avgNum': RRef,\n",
    "    'dwellTime': dwellTime,\n",
    "    'gradAmp': gradAmp,\n",
    "    'kspace_all': datanegy_FT,  \n",
    "    'nch': nch,\n",
    "    'roPts': roPts,\n",
    "    'roTime': roTime\n",
    "}\n",
    "filename_neg = f\"/Users/jamesbacon/Library/CloudStorage/OneDrive-Nexus365/GIRF_Correction/250313_pe_girf/GIRF_PE/Ref-{direction}slice.mat\"\n",
    "#savemat(filename_neg, MatDataRef_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_data = data_tri_corrected.reshape(18, 100, 80000)\n",
    "\n",
    "# Initialize a list to hold the 4 groups\n",
    "groups = []\n",
    "\n",
    "# File paths for saving the .mat files\n",
    "file_paths = [\n",
    "    f'/Users/jamesbacon/Library/CloudStorage/OneDrive-Nexus365/GIRF_Correction/250313_pe_girf/GIRF_PE/Positive+{direction}slice.mat',\n",
    "    f'/Users/jamesbacon/Library/CloudStorage/OneDrive-Nexus365/GIRF_Correction/250313_pe_girf/GIRF_PE/Positive-{direction}slice.mat',\n",
    "    f'/Users/jamesbacon/Library/CloudStorage/OneDrive-Nexus365/GIRF_Correction/250313_pe_girf/GIRF_PE/Negative+{direction}slice.mat',\n",
    "    f'/Users/jamesbacon/Library/CloudStorage/OneDrive-Nexus365/GIRF_Correction/250313_pe_girf/GIRF_PE/Negative-{direction}slice.mat'\n",
    "]\n",
    "\n",
    "def tri_2dFT(input):\n",
    "    input_grid = input.reshape(80000, 5, 5, 18)\n",
    "    ft_kspace = ft_kspace = np.fft.fft2(input_grid, axes=(1, 2))\n",
    "    ft_kspace_shifted = np.fft.fftshift(ft_kspace, axes=(1, 2))\n",
    "    modified_kspace_all = ft_kspace_shifted.reshape(80000, 25, 18)\n",
    "\n",
    "    return modified_kspace_all\n",
    "\n",
    "# Process each group and save as individual .mat files\n",
    "for i in range(4):\n",
    "    # For each group, select the appropriate readouts based on the alternating pattern (0,1,2,3,...)\n",
    "    selected_data = reshaped_data[:, i::4, :]\n",
    "    \n",
    "    # Now, we need to reshape this selected data into the shape (80000, 25, 18)\n",
    "    group_data = selected_data.transpose(2, 1, 0)  # This gives us the shape (80000, 25, 18)\n",
    "\n",
    "    group_data_FT = tri_2dFT(group_data)\n",
    "    \n",
    "    # Create the dictionary with metadata\n",
    "    MatDataTri = {\n",
    "        'acqNum': acqNumTri,\n",
    "        'avgNum': RTri,\n",
    "        'dwellTime': dwellTime,\n",
    "        'gradAmp': gradAmp,\n",
    "        'kspace_all': group_data_FT,  # kspace array is now the group data\n",
    "        'nch': nch,\n",
    "        'roPts': roPts,\n",
    "        'roTime': roTime\n",
    "    }\n",
    "    \n",
    "    # Save the dictionary along with the group data in the corresponding .mat file\n",
    "    #savemat(file_paths[i],  MatDataTri)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsl_mrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
